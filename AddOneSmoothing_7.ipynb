{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzVPsekQ6OrZnQyLeMhjjv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PrHXRBQYO-hP","executionInfo":{"status":"ok","timestamp":1746461884977,"user_tz":-330,"elapsed":53,"user":{"displayName":"PALLAV PRABHASH","userId":"13519120397589374682"}},"outputId":"c75144d5-acea-40cb-b11d-5ca084fbd0d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Bigram probabilities with Add-One Smoothing:\n","P(quick | the) = 0.181818\n","P(brown | quick) = 0.200000\n","P(fox | brown) = 0.200000\n","P(jumps | fox) = 0.200000\n","P(over | jumps) = 0.200000\n","P(the | over) = 0.200000\n","P(lazy | the) = 0.181818\n","P(dog | lazy) = 0.200000\n","P(. | dog) = 0.200000\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.util import bigrams\n","from nltk import FreqDist\n","from collections import defaultdict\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Updated sentence (slightly longer)\n","sentence = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the sentence and convert to lowercase\n","words = nltk.word_tokenize(sentence.lower())\n","\n","# Generate bigrams from the tokenized words\n","bigrams_list = list(bigrams(words))\n","\n","# Calculate the frequency distribution of bigrams\n","bigram_freq = FreqDist(bigrams_list)\n","\n","# Create vocabulary (set of unique words) and calculate its size (V)\n","vocabulary = set(words)\n","V = len(vocabulary)\n","\n","# Calculate the frequency distribution of individual words\n","word_freq = FreqDist(words)\n","\n","# Initialize defaultdict to store the smoothed probabilities\n","smoothed_probabilities = defaultdict(float)\n","\n","# Apply Add-One Smoothing to calculate bigram probabilities\n","for bigram, count in bigram_freq.items():\n","    w1, w2 = bigram\n","    smoothed_prob = (count + 1) / (word_freq[w1] + V)\n","    smoothed_probabilities[bigram] = smoothed_prob\n","\n","# Print the smoothed probabilities for each bigram\n","print(\"Bigram probabilities with Add-One Smoothing:\")\n","for bigram, smoothed_prob in smoothed_probabilities.items():\n","    print(f\"P({bigram[1]} | {bigram[0]}) = {smoothed_prob:.6f}\")\n"]}]}